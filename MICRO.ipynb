{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaSQB2KTESPmqJCftJvV4y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aSafarpoor/storehouse/blob/main/MICRO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is **MIRCO** implemantation from https://github.com/CRIPAC-DIG/MICRO/blob/main/codes/Models.py which is a paper **\"Latent Structure Mining with Contrastive Modality Fusion for Multimedia Recommendation\"**"
      ],
      "metadata": {
        "id": "xmp8YcvTJTMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#how to use based on MICRO wiki:\n",
        "\n",
        "Start training and inference as:\n",
        "```\n",
        "cd codes\n",
        "python main.py --dataset {DATASET}\n",
        "```\n",
        "For cold-start settings:\n",
        "```\n",
        "python main.py --dataset {DATASET} --core 0 --verbose 1 --lr 1e-5\n",
        "```"
      ],
      "metadata": {
        "id": "s-M9c6y9Ssz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# requarements:\n",
        "* Python 3.6\n",
        "* torch==1.5.0\n",
        "* scikit-learn==0.24.2\n",
        "* torch-scatter==2.0.8"
      ],
      "metadata": {
        "id": "aGj3ITh-KW4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch==1.5.0\n",
        "# !pip install torch-scatter==2.0.8\n",
        "\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDMihBFbKWJ_",
        "outputId": "a72c4efb-b2bb-4fc6-f7af-4d346b09af5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 8.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 9.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 10.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (709 kB)\n",
            "\u001b[K     |████████████████████████████████| 709 kB 9.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=18acd1dfe47370051654585f2d2da7c458f04a9827da84ac0fa28f7fb87f1437\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.1.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mount drive"
      ],
      "metadata": {
        "id": "xl8pt0rpT6wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# drive.flush_and_unmount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI-DulYuT-Zo",
        "outputId": "f4c80897-848a-4fa0-fddc-a5b3766c24d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/CRIPAC-DIG/MICRO/archive/refs/heads/main.zip\n",
        "!unzip main.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU0NcHQFCRjF",
        "outputId": "94e66a7a-41a7-4397-d2ab-0d9d6d37f88e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-23 20:30:25--  https://github.com/CRIPAC-DIG/MICRO/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/CRIPAC-DIG/MICRO/zip/refs/heads/main [following]\n",
            "--2022-09-23 20:30:25--  https://codeload.github.com/CRIPAC-DIG/MICRO/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.121.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.121.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘main.zip’\n",
            "\n",
            "main.zip                [ <=>                ]  17.65K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2022-09-23 20:30:25 (2.25 MB/s) - ‘main.zip’ saved [18077]\n",
            "\n",
            "Archive:  main.zip\n",
            "3d2e61a91a4c6e2e8cefa89b85e7f17e50983da8\n",
            "   creating: MICRO-main/\n",
            "  inflating: MICRO-main/.gitignore   \n",
            "  inflating: MICRO-main/LICENSE      \n",
            "  inflating: MICRO-main/README.md    \n",
            "   creating: MICRO-main/codes/\n",
            "  inflating: MICRO-main/codes/Models.py  \n",
            "  inflating: MICRO-main/codes/main.py  \n",
            "   creating: MICRO-main/codes/utility/\n",
            "  inflating: MICRO-main/codes/utility/batch_test.py  \n",
            "  inflating: MICRO-main/codes/utility/load_data.py  \n",
            "  inflating: MICRO-main/codes/utility/logging.py  \n",
            "  inflating: MICRO-main/codes/utility/metrics.py  \n",
            "  inflating: MICRO-main/codes/utility/norm.py  \n",
            "  inflating: MICRO-main/codes/utility/parser.py  \n",
            "   creating: MICRO-main/data/\n",
            "  inflating: MICRO-main/data/build_data.py  \n",
            "  inflating: MICRO-main/data/cold_start.py  \n",
            "   creating: MICRO-main/data/sentence-bert/\n",
            " extracting: MICRO-main/data/sentence-bert/.placeholder  \n",
            "   creating: MICRO-main/data/sports/\n",
            "   creating: MICRO-main/data/sports/meta-data/\n",
            " extracting: MICRO-main/data/sports/meta-data/.placeholder  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv MICRO-main micro"
      ],
      "metadata": {
        "id": "GoO-sbbfC9lL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm main.zip"
      ],
      "metadata": {
        "id": "zwPuuWVfDI_j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# % mkdir micro\n",
        "%cd micro\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XJkisu5Yxem",
        "outputId": "44e22714-b9a3-45b1-aad6-5d09f1630a76"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/micro\n",
            "codes  data  LICENSE  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils:"
      ],
      "metadata": {
        "id": "o3MhKn97P1cJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## logging"
      ],
      "metadata": {
        "id": "Hqiib3xbRVlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "class Logger():\n",
        "    def __init__(self, filename, is_debug, path='./logs/'):\n",
        "        self.filename = filename\n",
        "        self.path = path\n",
        "        self.log_ = not is_debug\n",
        "    def logging(self, s):\n",
        "        s = str(s)\n",
        "        print(datetime.now().strftime('%Y-%m-%d %H:%M: '), s)\n",
        "        if self.log_:\n",
        "            with open(os.path.join(os.path.join(self.path, self.filename)), 'a+') as f_log:\n",
        "                f_log.write(str(datetime.now().strftime('%Y-%m-%d %H:%M:  ')) + s + '\\n')\n"
      ],
      "metadata": {
        "id": "SjRD7ZaPRVPK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## metrics"
      ],
      "metadata": {
        "id": "2TGNv0isQZ_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def recall(rank, ground_truth, N):\n",
        "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
        "\n",
        "\n",
        "def precision_at_k(r, k):\n",
        "    \"\"\"Score is precision @ k\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "    Returns:\n",
        "        Precision @ k\n",
        "    Raises:\n",
        "        ValueError: len(r) must be >= k\n",
        "    \"\"\"\n",
        "    assert k >= 1\n",
        "    r = np.asarray(r)[:k]\n",
        "    return np.mean(r)\n",
        "\n",
        "\n",
        "def average_precision(r,cut):\n",
        "    \"\"\"Score is average precision (area under PR curve)\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "    Returns:\n",
        "        Average precision\n",
        "    \"\"\"\n",
        "    r = np.asarray(r)\n",
        "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
        "    if not out:\n",
        "        return 0.\n",
        "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
        "\n",
        "\n",
        "def mean_average_precision(rs):\n",
        "    \"\"\"Score is mean average precision\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "    Returns:\n",
        "        Mean average precision\n",
        "    \"\"\"\n",
        "    return np.mean([average_precision(r) for r in rs])\n",
        "\n",
        "\n",
        "def dcg_at_k(r, k, method=1):\n",
        "    \"\"\"Score is discounted cumulative gain (dcg)\n",
        "    Relevance is positive real values.  Can use binary\n",
        "    as the previous methods.\n",
        "    Returns:\n",
        "        Discounted cumulative gain\n",
        "    \"\"\"\n",
        "    r = np.asfarray(r)[:k]\n",
        "    if r.size:\n",
        "        if method == 0:\n",
        "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
        "        elif method == 1:\n",
        "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
        "        else:\n",
        "            raise ValueError('method must be 0 or 1.')\n",
        "    return 0.\n",
        "\n",
        "\n",
        "def ndcg_at_k(r, k, method=1):\n",
        "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
        "    Relevance is positive real values.  Can use binary\n",
        "    as the previous methods.\n",
        "    Returns:\n",
        "        Normalized discounted cumulative gain\n",
        "    \"\"\"\n",
        "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
        "    if not dcg_max:\n",
        "        return 0.\n",
        "    return dcg_at_k(r, k, method) / dcg_max\n",
        "\n",
        "\n",
        "def recall_at_k(r, k, all_pos_num):\n",
        "    r = np.asfarray(r)[:k]\n",
        "    if all_pos_num == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return np.sum(r) / all_pos_num\n",
        "\n",
        "\n",
        "def hit_at_k(r, k):\n",
        "    r = np.array(r)[:k]\n",
        "    if np.sum(r) > 0:\n",
        "        return 1.\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "def F1(pre, rec):\n",
        "    if pre + rec > 0:\n",
        "        return (2.0 * pre * rec) / (pre + rec)\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "def auc(ground_truth, prediction):\n",
        "    try:\n",
        "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
        "    except Exception:\n",
        "        res = 0.\n",
        "    return res"
      ],
      "metadata": {
        "id": "0bs-idhIQcun"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## parser"
      ],
      "metadata": {
        "id": "LxiU8m4fQhtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"\")\n",
        "\n",
        "    parser.add_argument('--data_path', nargs='?', default='../data/',\n",
        "                        help='Input data path.')\n",
        "    parser.add_argument('--seed', type=int, default=123,\n",
        "                        help='Random seed')\n",
        "    parser.add_argument('--dataset', nargs='?', default='baby',\n",
        "                        help='Choose a dataset from {sports, baby, clothing}')\n",
        "    parser.add_argument('--verbose', type=int, default=5,\n",
        "                        help='Interval of evaluation.')\n",
        "    parser.add_argument('--epoch', type=int, default=1000,\n",
        "                        help='Number of epoch.')\n",
        "    parser.add_argument('--batch_size', type=int, default=1024,\n",
        "                        help='Batch size.')\n",
        "    parser.add_argument('--regs', nargs='?', default='[1e-5,1e-5,1e-2]',\n",
        "                        help='Regularizations.')\n",
        "    parser.add_argument('--lr', type=float, default=0.0005,\n",
        "                        help='Learning rate.')\n",
        "\n",
        "    parser.add_argument('--embed_size', type=int, default=64,\n",
        "                        help='Embedding size.')                     \n",
        "    parser.add_argument('--weight_size', nargs='?', default='[64,64]',\n",
        "                        help='Output sizes of every layer')\n",
        "    parser.add_argument('--core', type=int, default=5,\n",
        "                        help='5-core for warm-start; 0-core for cold start')\n",
        "    parser.add_argument('--topk', type=int, default=10,\n",
        "                        help='K value of k-NN sparsification')  \n",
        "    parser.add_argument('--lambda_coeff', type=float, default=0.9,\n",
        "                        help='Lambda value of skip connection')\n",
        "    parser.add_argument('--cf_model', nargs='?', default='lightgcn',\n",
        "                        help='Downstream Collaborative Filtering model {mf, ngcf, lightgcn}')   \n",
        "    parser.add_argument('--early_stopping_patience', type=int, default=10,\n",
        "                        help='') \n",
        "    parser.add_argument('--layers', type=int, default=1,\n",
        "                        help='Number of item graph conv layers')  \n",
        "    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1]',\n",
        "                        help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
        "\n",
        "    parser.add_argument('--sparse', type=int, default=1, help='Sparse or dense adjacency matrix')   \n",
        "    parser.add_argument('--debug', action='store_true')  \n",
        "    parser.add_argument('--loss_ratio', type=float, default=0.03, help='Control the effect of the contrastive auxiliary task')        \n",
        "    parser.add_argument('--norm_type', nargs='?', default='sym', help='Adjacency matrix normalization operation') \n",
        "    parser.add_argument('--gpu_id', type=int, default=1,\n",
        "                        help='GPU id')\n",
        "    parser.add_argument('--Ks', nargs='?', default='[10, 20]',\n",
        "                        help='K value of ndcg/recall @ k')\n",
        "    parser.add_argument('--test_flag', nargs='?', default='part',\n",
        "                        help='Specify the test type from {part, full}, indicating whether the reference is done in mini-batch')\n",
        "\n",
        "            \n",
        "\n",
        "    return parser.parse_args()\n"
      ],
      "metadata": {
        "id": "PEmT0l1LQkdj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## norm"
      ],
      "metadata": {
        "id": "ZmdHfK5RQyOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def build_sim(context):\n",
        "    context_norm = context.div(torch.norm(context, p=2, dim=-1, keepdim=True))\n",
        "    sim = torch.mm(context_norm, context_norm.transpose(1, 0))\n",
        "    return sim\n",
        "\n",
        "def build_knn_normalized_graph(adj, topk, is_sparse, norm_type):\n",
        "    device = adj.device\n",
        "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
        "    if is_sparse:\n",
        "        tuple_list = [[row, int(col)] for row in range(len(knn_ind)) for col in knn_ind[row]]\n",
        "        row = [i[0] for i in tuple_list]\n",
        "        col = [i[1] for i in tuple_list]\n",
        "        i = torch.LongTensor([row, col]).to(device)\n",
        "        v = knn_val.flatten()\n",
        "        edge_index, edge_weight = get_sparse_laplacian(i, v, normalization=norm_type, num_nodes=adj.shape[0])\n",
        "        return torch.sparse_coo_tensor(edge_index, edge_weight, adj.shape)\n",
        "    else:\n",
        "        weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
        "        return get_dense_laplacian(weighted_adjacency_matrix, normalization=norm_type)\n",
        "\n",
        "def get_sparse_laplacian(edge_index, edge_weight, num_nodes, normalization='none'):\n",
        "    from torch_scatter import scatter_add\n",
        "    row, col = edge_index[0], edge_index[1]\n",
        "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "\n",
        "    if normalization == 'sym':\n",
        "        deg_inv_sqrt = deg.pow_(-0.5)\n",
        "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
        "        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
        "    elif normalization == 'rw':\n",
        "        deg_inv = 1.0 / deg\n",
        "        deg_inv.masked_fill_(deg_inv == float('inf'), 0)\n",
        "        edge_weight = deg_inv[row] * edge_weight\n",
        "    return edge_index, edge_weight\n",
        "\n",
        "\n",
        "def get_dense_laplacian(adj, normalization='none'):\n",
        "    if normalization == 'sym':\n",
        "        rowsum = torch.sum(adj, -1)\n",
        "        d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
        "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
        "        d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
        "        L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
        "    elif normalization == 'rw':\n",
        "        rowsum = torch.sum(adj, -1)\n",
        "        d_inv = torch.pow(rowsum, -1)\n",
        "        d_inv[torch.isinf(d_inv)] = 0.\n",
        "        d_mat_inv = torch.diagflat(d_inv)\n",
        "        L_norm = torch.mm(d_mat_inv, adj)\n",
        "    elif normalization == 'none':\n",
        "        L_norm = adj\n",
        "    return L_norm\n"
      ],
      "metadata": {
        "id": "lW_w--E0Q0FS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load_data"
      ],
      "metadata": {
        "id": "TE5Xx7D-Qn3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random as rd\n",
        "import scipy.sparse as sp\n",
        "from time import time\n",
        "import json\n",
        "# from utility.parser import parse_args\n",
        "args = parse_args()\n",
        "\n",
        "class Data(object):\n",
        "    def __init__(self, path, batch_size):\n",
        "        self.path = path + '/%d-core' % args.core\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        train_file = path + '/%d-core/train.json' % (args.core)\n",
        "        val_file = path + '/%d-core/val.json' % (args.core)\n",
        "        test_file = path + '/%d-core/test.json'  % (args.core)\n",
        "\n",
        "        #get number of users and items\n",
        "        self.n_users, self.n_items = 0, 0\n",
        "        self.n_train, self.n_test = 0, 0\n",
        "        self.neg_pools = {}\n",
        "\n",
        "        self.exist_users = []\n",
        "\n",
        "        train = json.load(open(train_file))\n",
        "        test = json.load(open(test_file))\n",
        "        val = json.load(open(val_file))\n",
        "        for uid, items in train.items():\n",
        "            if len(items) == 0:\n",
        "                continue\n",
        "            uid = int(uid)\n",
        "            self.exist_users.append(uid)\n",
        "            self.n_items = max(self.n_items, max(items))\n",
        "            self.n_users = max(self.n_users, uid)\n",
        "            self.n_train += len(items)\n",
        "\n",
        "        for uid, items in test.items():\n",
        "            uid = int(uid)\n",
        "            try:\n",
        "                self.n_items = max(self.n_items, max(items))\n",
        "                self.n_test += len(items)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        for uid, items in val.items():\n",
        "            uid = int(uid)\n",
        "            try:\n",
        "                self.n_items = max(self.n_items, max(items))\n",
        "                self.n_val += len(items)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        self.n_items += 1\n",
        "        self.n_users += 1\n",
        "\n",
        "        self.print_statistics()\n",
        "\n",
        "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
        "        self.R_Item_Interacts = sp.dok_matrix((self.n_items, self.n_items), dtype=np.float32)\n",
        "\n",
        "        self.train_items, self.test_set, self.val_set = {}, {}, {}\n",
        "        for uid, train_items in train.items():\n",
        "            if len(train_items) == 0:\n",
        "                continue\n",
        "            uid = int(uid)\n",
        "            for idx, i in enumerate(train_items):\n",
        "                self.R[uid, i] = 1.\n",
        "\n",
        "            self.train_items[uid] = train_items\n",
        "\n",
        "        for uid, test_items in test.items():\n",
        "            uid = int(uid)\n",
        "            if len(test_items) == 0:\n",
        "                continue\n",
        "            try:\n",
        "                self.test_set[uid] = test_items\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        for uid, val_items in val.items():\n",
        "            uid = int(uid)\n",
        "            if len(val_items) == 0:\n",
        "                continue\n",
        "            try:\n",
        "                self.val_set[uid] = val_items\n",
        "            except:\n",
        "                continue            \n",
        "\n",
        "    def get_adj_mat(self):\n",
        "        try:\n",
        "            t1 = time()\n",
        "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
        "            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n",
        "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
        "            print('already load adj matrix', adj_mat.shape, time() - t1)\n",
        "\n",
        "        except Exception:\n",
        "            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n",
        "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
        "            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n",
        "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
        "        return adj_mat, norm_adj_mat, mean_adj_mat\n",
        "\n",
        "    def create_adj_mat(self):\n",
        "        t1 = time()\n",
        "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
        "        adj_mat = adj_mat.tolil()\n",
        "        R = self.R.tolil()\n",
        "\n",
        "        adj_mat[:self.n_users, self.n_users:] = R\n",
        "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
        "        adj_mat = adj_mat.todok()\n",
        "        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n",
        "\n",
        "        t2 = time()\n",
        "\n",
        "        def normalized_adj_single(adj):\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "\n",
        "            d_inv = np.power(rowsum, -1).flatten()\n",
        "            d_inv[np.isinf(d_inv)] = 0.\n",
        "            d_mat_inv = sp.diags(d_inv)\n",
        "\n",
        "            norm_adj = d_mat_inv.dot(adj)\n",
        "            # norm_adj = adj.dot(d_mat_inv)\n",
        "            print('generate single-normalized adjacency matrix.')\n",
        "            return norm_adj.tocoo()\n",
        "\n",
        "        def get_D_inv(adj):\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "\n",
        "            d_inv = np.power(rowsum, -1).flatten()\n",
        "            d_inv[np.isinf(d_inv)] = 0.\n",
        "            d_mat_inv = sp.diags(d_inv)\n",
        "            return d_mat_inv\n",
        "\n",
        "        def check_adj_if_equal(adj):\n",
        "            dense_A = np.array(adj.todense())\n",
        "            degree = np.sum(dense_A, axis=1, keepdims=False)\n",
        "\n",
        "            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
        "            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n",
        "            return temp\n",
        "\n",
        "        norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
        "        mean_adj_mat = normalized_adj_single(adj_mat)\n",
        "\n",
        "        print('already normalize adjacency matrix', time() - t2)\n",
        "        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        if self.batch_size <= self.n_users:\n",
        "            users = rd.sample(self.exist_users, self.batch_size)\n",
        "        else:\n",
        "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
        "        # users = self.exist_users[:]\n",
        "\n",
        "        def sample_pos_items_for_u(u, num):\n",
        "            pos_items = self.train_items[u]\n",
        "            n_pos_items = len(pos_items)\n",
        "            pos_batch = []\n",
        "            while True:\n",
        "                if len(pos_batch) == num: break\n",
        "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
        "                pos_i_id = pos_items[pos_id]\n",
        "\n",
        "                if pos_i_id not in pos_batch:\n",
        "                    pos_batch.append(pos_i_id)\n",
        "            return pos_batch\n",
        "\n",
        "        def sample_neg_items_for_u(u, num):\n",
        "            neg_items = []\n",
        "            while True:\n",
        "                if len(neg_items) == num: break\n",
        "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
        "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
        "                    neg_items.append(neg_id)\n",
        "            return neg_items\n",
        "\n",
        "        def sample_neg_items_for_u_from_pools(u, num):\n",
        "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
        "            return rd.sample(neg_items, num)\n",
        "\n",
        "        pos_items, neg_items = [], []\n",
        "        for u in users:\n",
        "            pos_items += sample_pos_items_for_u(u, 1)\n",
        "            neg_items += sample_neg_items_for_u(u, 1)\n",
        "            # neg_items += sample_neg_items_for_u(u, 3)\n",
        "        return users, pos_items, neg_items\n",
        "\n",
        "\n",
        "\n",
        "    def print_statistics(self):\n",
        "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
        "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
        "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "jL4aRzvTQr-I",
        "outputId": "4c3b70cb-b4e6-4b35-b9c0-fdfb69b3c595"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--data_path [DATA_PATH]] [--seed SEED]\n",
            "                             [--dataset [DATASET]] [--verbose VERBOSE]\n",
            "                             [--epoch EPOCH] [--batch_size BATCH_SIZE]\n",
            "                             [--regs [REGS]] [--lr LR]\n",
            "                             [--embed_size EMBED_SIZE]\n",
            "                             [--weight_size [WEIGHT_SIZE]] [--core CORE]\n",
            "                             [--topk TOPK] [--lambda_coeff LAMBDA_COEFF]\n",
            "                             [--cf_model [CF_MODEL]]\n",
            "                             [--early_stopping_patience EARLY_STOPPING_PATIENCE]\n",
            "                             [--layers LAYERS] [--mess_dropout [MESS_DROPOUT]]\n",
            "                             [--sparse SPARSE] [--debug]\n",
            "                             [--loss_ratio LOSS_RATIO]\n",
            "                             [--norm_type [NORM_TYPE]] [--gpu_id GPU_ID]\n",
            "                             [--Ks [KS]] [--test_flag [TEST_FLAG]]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-0a5d5007-9f76-45b1-ad67-016838cbb703.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## batch_test"
      ],
      "metadata": {
        "id": "i3Vd5Ot9P_on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import utility.metrics as metrics\n",
        "# from utility.parser import parse_args\n",
        "# from utility.load_data import Data\n",
        "import multiprocessing\n",
        "import heapq\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "cores = multiprocessing.cpu_count() // 5\n",
        "\n",
        "args = parse_args()\n",
        "Ks = eval(args.Ks)\n",
        "\n",
        "data_generator = Data(path=args.data_path + args.dataset, batch_size=args.batch_size)\n",
        "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
        "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
        "BATCH_SIZE = args.batch_size\n",
        "\n",
        "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
        "    item_score = {}\n",
        "    for i in test_items:\n",
        "        item_score[i] = rating[i]\n",
        "\n",
        "    K_max = max(Ks)\n",
        "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
        "\n",
        "    r = []\n",
        "    for i in K_max_item_score:\n",
        "        if i in user_pos_test:\n",
        "            r.append(1)\n",
        "        else:\n",
        "            r.append(0)\n",
        "    auc = 0.\n",
        "    return r, auc\n",
        "\n",
        "def get_auc(item_score, user_pos_test):\n",
        "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
        "    item_score.reverse()\n",
        "    item_sort = [x[0] for x in item_score]\n",
        "    posterior = [x[1] for x in item_score]\n",
        "\n",
        "    r = []\n",
        "    for i in item_sort:\n",
        "        if i in user_pos_test:\n",
        "            r.append(1)\n",
        "        else:\n",
        "            r.append(0)\n",
        "    auc = metrics.auc(ground_truth=r, prediction=posterior)\n",
        "    return auc\n",
        "\n",
        "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
        "    item_score = {}\n",
        "    for i in test_items:\n",
        "        item_score[i] = rating[i]\n",
        "\n",
        "    K_max = max(Ks)\n",
        "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
        "\n",
        "    r = []\n",
        "    for i in K_max_item_score:\n",
        "        if i in user_pos_test:\n",
        "            r.append(1)\n",
        "        else:\n",
        "            r.append(0)\n",
        "    auc = get_auc(item_score, user_pos_test)\n",
        "    return r, auc\n",
        "\n",
        "def get_performance(user_pos_test, r, auc, Ks):\n",
        "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
        "\n",
        "    for K in Ks:\n",
        "        precision.append(metrics.precision_at_k(r, K))\n",
        "        recall.append(metrics.recall_at_k(r, K, len(user_pos_test)))\n",
        "        ndcg.append(metrics.ndcg_at_k(r, K))\n",
        "        hit_ratio.append(metrics.hit_at_k(r, K))\n",
        "\n",
        "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
        "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}\n",
        "\n",
        "\n",
        "def test_one_user(x):\n",
        "    # user u's ratings for user u\n",
        "    is_val = x[-1]\n",
        "    rating = x[0]\n",
        "    #uid\n",
        "    u = x[1]\n",
        "    #user u's items in the training set\n",
        "    try:\n",
        "        training_items = data_generator.train_items[u]\n",
        "    except Exception:\n",
        "        training_items = []\n",
        "    #user u's items in the test set\n",
        "    if is_val:\n",
        "        user_pos_test = data_generator.val_set[u]\n",
        "    else:\n",
        "        user_pos_test = data_generator.test_set[u]\n",
        "\n",
        "    all_items = set(range(ITEM_NUM))\n",
        "\n",
        "    test_items = list(all_items - set(training_items))\n",
        "\n",
        "    if args.test_flag == 'part':\n",
        "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
        "    else:\n",
        "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
        "\n",
        "    return get_performance(user_pos_test, r, auc, Ks)\n",
        "\n",
        "\n",
        "def test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val, drop_flag=False, batch_test_flag=False):\n",
        "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
        "              'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
        "    pool = multiprocessing.Pool(cores)\n",
        "\n",
        "    u_batch_size = BATCH_SIZE * 2\n",
        "    i_batch_size = BATCH_SIZE\n",
        "\n",
        "    test_users = users_to_test\n",
        "    n_test_users = len(test_users)\n",
        "    n_user_batchs = n_test_users // u_batch_size + 1\n",
        "    count = 0\n",
        "\n",
        "    for u_batch_id in range(n_user_batchs):\n",
        "        start = u_batch_id * u_batch_size\n",
        "        end = (u_batch_id + 1) * u_batch_size\n",
        "        user_batch = test_users[start: end]\n",
        "        if batch_test_flag:\n",
        "            n_item_batchs = ITEM_NUM // i_batch_size + 1\n",
        "            rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
        "\n",
        "            i_count = 0\n",
        "            for i_batch_id in range(n_item_batchs):\n",
        "                i_start = i_batch_id * i_batch_size\n",
        "                i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
        "\n",
        "                item_batch = range(i_start, i_end)\n",
        "                u_g_embeddings = ua_embeddings[user_batch]\n",
        "                i_g_embeddings = ia_embeddings[item_batch]\n",
        "                i_rate_batch = torch.matmul(u_g_embeddings, torch.transpose(i_g_embeddings, 0, 1))\n",
        "\n",
        "                rate_batch[:, i_start: i_end] = i_rate_batch\n",
        "                i_count += i_rate_batch.shape[1]\n",
        "\n",
        "            assert i_count == ITEM_NUM\n",
        "\n",
        "        else:\n",
        "            item_batch = range(ITEM_NUM)\n",
        "            u_g_embeddings = ua_embeddings[user_batch]\n",
        "            i_g_embeddings = ia_embeddings[item_batch]\n",
        "            rate_batch = torch.matmul(u_g_embeddings, torch.transpose(i_g_embeddings, 0, 1))\n",
        "\n",
        "        rate_batch = rate_batch.detach().cpu().numpy()\n",
        "        user_batch_rating_uid = zip(rate_batch, user_batch, [is_val] * len(user_batch))\n",
        "\n",
        "        batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
        "        count += len(batch_result)\n",
        "\n",
        "        for re in batch_result:\n",
        "            result['precision'] += re['precision'] / n_test_users\n",
        "            result['recall'] += re['recall'] / n_test_users\n",
        "            result['ndcg'] += re['ndcg'] / n_test_users\n",
        "            result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
        "            result['auc'] += re['auc'] / n_test_users\n",
        "\n",
        "    assert count == n_test_users\n",
        "    pool.close()\n",
        "    return result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "kYT0WKVLP1AP",
        "outputId": "f1e1d58b-174a-48bd-d508-27ae3e3f2ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--data_path [DATA_PATH]] [--seed SEED]\n",
            "                             [--dataset [DATASET]] [--verbose VERBOSE]\n",
            "                             [--epoch EPOCH] [--batch_size BATCH_SIZE]\n",
            "                             [--regs [REGS]] [--lr LR]\n",
            "                             [--embed_size EMBED_SIZE]\n",
            "                             [--weight_size [WEIGHT_SIZE]] [--core CORE]\n",
            "                             [--topk TOPK] [--lambda_coeff LAMBDA_COEFF]\n",
            "                             [--cf_model [CF_MODEL]]\n",
            "                             [--early_stopping_patience EARLY_STOPPING_PATIENCE]\n",
            "                             [--layers LAYERS] [--mess_dropout [MESS_DROPOUT]]\n",
            "                             [--sparse SPARSE] [--debug]\n",
            "                             [--loss_ratio LOSS_RATIO]\n",
            "                             [--norm_type [NORM_TYPE]] [--gpu_id GPU_ID]\n",
            "                             [--Ks [KS]] [--test_flag [TEST_FLAG]]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-21d6a4de-fcbe-441c-a723-28cfd7125bd5.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# models"
      ],
      "metadata": {
        "id": "NWLg5v6xRqzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.sparse as sparse\n",
        "\n",
        "# from utility.parser import parse_args\n",
        "# from Models import MICRO\n",
        "# from utility.batch_test import *\n",
        "# from utility.logging import Logger\n",
        "\n",
        "args = parse_args()\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, data_config):\n",
        "        # argument settings\n",
        "        self.n_users = data_config['n_users']\n",
        "        self.n_items = data_config['n_items']\n",
        "        self.task_name = \"%s_%s_%s\" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), args.dataset, args.cf_model,)\n",
        "        self.logger = Logger(filename=self.task_name, is_debug=args.debug)\n",
        "        self.logger.logging(\"PID: %d\" % os.getpid())\n",
        "        self.logger.logging(str(args))\n",
        "\n",
        "        self.mess_dropout = eval(args.mess_dropout)\n",
        "        self.lr = args.lr\n",
        "        self.emb_dim = args.embed_size\n",
        "        self.batch_size = args.batch_size\n",
        "        self.weight_size = eval(args.weight_size)\n",
        "        self.n_layers = len(self.weight_size)\n",
        "        self.regs = eval(args.regs)\n",
        "        self.decay = self.regs[0]\n",
        "\n",
        "        self.norm_adj = data_config['norm_adj']\n",
        "        self.norm_adj = self.sparse_mx_to_torch_sparse_tensor(self.norm_adj).float().cuda()\n",
        "        \n",
        "        image_feats = np.load('../data/{}/image_feat.npy'.format(args.dataset))\n",
        "        text_feats = np.load('../data/{}/text_feat.npy'.format(args.dataset))\n",
        "\n",
        "        self.model = MICRO(self.n_users, self.n_items, self.emb_dim, self.weight_size, self.mess_dropout, image_feats, text_feats)                      \n",
        "        self.model = self.model.cuda()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.lr_scheduler = self.set_lr_scheduler()\n",
        "\n",
        "    def set_lr_scheduler(self):\n",
        "        fac = lambda epoch: 0.96 ** (epoch / 50)\n",
        "        scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=fac)\n",
        "        return scheduler\n",
        "\n",
        "    def test(self, users_to_test, is_val):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            ua_embeddings, ia_embeddings, *rest = self.model(self.norm_adj, build_item_graph=True)\n",
        "        result = test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val)\n",
        "        return result\n",
        "\n",
        "    def train(self):\n",
        "        training_time_list = []\n",
        "        loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []\n",
        "        stopping_step = 0\n",
        "        should_stop = False\n",
        "        cur_best_pre_0 = 0.\n",
        "\n",
        "        n_batch = data_generator.n_train // args.batch_size + 1\n",
        "        best_recall = 0\n",
        "        for epoch in (range(args.epoch)):\n",
        "            t1 = time()\n",
        "            loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n",
        "            contrastive_loss = 0.\n",
        "            n_batch = data_generator.n_train // args.batch_size + 1\n",
        "            f_time, b_time, loss_time, opt_time, clip_time, emb_time = 0., 0., 0., 0., 0., 0.\n",
        "            sample_time = 0.\n",
        "            build_item_graph = True\n",
        "            for idx in (range(n_batch)):\n",
        "                self.model.train()\n",
        "                self.optimizer.zero_grad()\n",
        "                sample_t1 = time()\n",
        "                users, pos_items, neg_items = data_generator.sample()\n",
        "                sample_time += time() - sample_t1                                                 \n",
        "                ua_embeddings, ia_embeddings, image_item_embeds, text_item_embeds, fusion_embed = self.model(self.norm_adj, build_item_graph=build_item_graph)\n",
        "                build_item_graph = False\n",
        "                u_g_embeddings = ua_embeddings[users]\n",
        "                pos_i_g_embeddings = ia_embeddings[pos_items]\n",
        "                neg_i_g_embeddings = ia_embeddings[neg_items]\n",
        "\n",
        "\n",
        "                batch_mf_loss, batch_emb_loss, batch_reg_loss = self.bpr_loss(u_g_embeddings, pos_i_g_embeddings,\n",
        "                                                                              neg_i_g_embeddings)\n",
        "\n",
        "                batch_contrastive_loss = 0\n",
        "                batch_contrastive_loss += self.model.batched_contrastive_loss(image_item_embeds,fusion_embed)\n",
        "                batch_contrastive_loss += self.model.batched_contrastive_loss(text_item_embeds,fusion_embed)\n",
        "\n",
        "                batch_contrastive_loss *=  args.loss_ratio\n",
        "                batch_loss = batch_mf_loss + batch_emb_loss + batch_reg_loss + batch_contrastive_loss\n",
        "\n",
        "                batch_loss.backward(retain_graph=False)\n",
        "                self.optimizer.step()\n",
        "\n",
        "                loss += float(batch_loss)\n",
        "                mf_loss += float(batch_mf_loss)\n",
        "                emb_loss += float(batch_emb_loss)\n",
        "                reg_loss += float(batch_reg_loss)\n",
        "                contrastive_loss += float(batch_contrastive_loss)\n",
        "\n",
        "\n",
        "            self.lr_scheduler.step()\n",
        "\n",
        "            del ua_embeddings, ia_embeddings, u_g_embeddings, neg_i_g_embeddings, pos_i_g_embeddings\n",
        "\n",
        "            if math.isnan(loss) == True:\n",
        "                self.logger.logging('ERROR: loss is nan.')\n",
        "                sys.exit()\n",
        "\n",
        "            if (epoch + 1) % args.verbose != 0:\n",
        "                perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f + %.5f]' % (\n",
        "                    epoch, time() - t1, loss, mf_loss, emb_loss, reg_loss)\n",
        "                training_time_list.append(time() - t1)\n",
        "                self.logger.logging(perf_str)\n",
        "                continue\n",
        "\n",
        "\n",
        "            t2 = time()\n",
        "            users_to_test = list(data_generator.test_set.keys())\n",
        "            users_to_val = list(data_generator.val_set.keys())\n",
        "            ret = self.test(users_to_val, is_val=True)\n",
        "            training_time_list.append(t2 - t1)\n",
        "\n",
        "            t3 = time()\n",
        "\n",
        "            loss_loger.append(loss)\n",
        "            rec_loger.append(ret['recall'])\n",
        "            pre_loger.append(ret['precision'])\n",
        "            ndcg_loger.append(ret['ndcg'])\n",
        "            hit_loger.append(ret['hit_ratio'])\n",
        "            if args.verbose > 0:\n",
        "                perf_str = 'Epoch %d [%.1fs + %.1fs]: train==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
        "                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f]' % \\\n",
        "                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, ret['recall'][0],\n",
        "                            ret['recall'][-1],\n",
        "                            ret['precision'][0], ret['precision'][-1], ret['hit_ratio'][0], ret['hit_ratio'][-1],\n",
        "                            ret['ndcg'][0], ret['ndcg'][-1])\n",
        "                self.logger.logging(perf_str)\n",
        "\n",
        "            if ret['recall'][1] > best_recall:\n",
        "                best_recall = ret['recall'][1]\n",
        "                test_ret = self.test(users_to_test, is_val=False)\n",
        "                self.logger.logging(\"Test_Recall@%d: %.5f\" % (eval(args.Ks)[1], test_ret['recall'][1]))\n",
        "                stopping_step = 0\n",
        "            elif stopping_step < args.early_stopping_patience:\n",
        "                stopping_step += 1\n",
        "                self.logger.logging('#####Early stopping steps: %d #####' % stopping_step)\n",
        "            else:\n",
        "                self.logger.logging('#####Early stop! #####')\n",
        "                break\n",
        "\n",
        "        self.logger.logging(str(test_ret))\n",
        "\n",
        "    def bpr_loss(self, users, pos_items, neg_items):\n",
        "        pos_scores = torch.sum(torch.mul(users, pos_items), dim=1)\n",
        "        neg_scores = torch.sum(torch.mul(users, neg_items), dim=1)\n",
        "\n",
        "        regularizer = 1./2*(users**2).sum() + 1./2*(pos_items**2).sum() + 1./2*(neg_items**2).sum()\n",
        "        regularizer = regularizer / self.batch_size\n",
        "\n",
        "        maxi = F.logsigmoid(pos_scores - neg_scores)\n",
        "        mf_loss = -torch.mean(maxi)\n",
        "\n",
        "        emb_loss = self.decay * regularizer\n",
        "        reg_loss = 0.0\n",
        "        return mf_loss, emb_loss, reg_loss\n",
        "\n",
        "    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):\n",
        "        \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "        sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "        indices = torch.from_numpy(\n",
        "            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "        values = torch.from_numpy(sparse_mx.data)\n",
        "        shape = torch.Size(sparse_mx.shape)\n",
        "        return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed) # cpu\n",
        "    torch.cuda.manual_seed_all(seed)  # gpu\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
        "    set_seed(args.seed)\n",
        "    config = dict()\n",
        "    config['n_users'] = data_generator.n_users\n",
        "    config['n_items'] = data_generator.n_items\n",
        "\n",
        "    plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()\n",
        "    config['norm_adj'] = norm_adj\n",
        "\n",
        "    trainer = Trainer(data_config=config)\n",
        "    trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "0W1rhboyRqMt",
        "outputId": "ae9f830f-fa0c-41ec-8339-215deffcb098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--data_path [DATA_PATH]] [--seed SEED]\n",
            "                             [--dataset [DATASET]] [--verbose VERBOSE]\n",
            "                             [--epoch EPOCH] [--batch_size BATCH_SIZE]\n",
            "                             [--regs [REGS]] [--lr LR]\n",
            "                             [--embed_size EMBED_SIZE]\n",
            "                             [--weight_size [WEIGHT_SIZE]] [--core CORE]\n",
            "                             [--topk TOPK] [--lambda_coeff LAMBDA_COEFF]\n",
            "                             [--cf_model [CF_MODEL]]\n",
            "                             [--early_stopping_patience EARLY_STOPPING_PATIENCE]\n",
            "                             [--layers LAYERS] [--mess_dropout [MESS_DROPOUT]]\n",
            "                             [--sparse SPARSE] [--debug]\n",
            "                             [--loss_ratio LOSS_RATIO]\n",
            "                             [--norm_type [NORM_TYPE]] [--gpu_id GPU_ID]\n",
            "                             [--Ks [KS]] [--test_flag [TEST_FLAG]]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-21d6a4de-fcbe-441c-a723-28cfd7125bd5.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "sPAZ0eYvQ-Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "8xWc-5zqKQKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from utility.parser import parse_args\n",
        "# from utility.norm import build_sim, build_knn_normalized_graph"
      ],
      "metadata": {
        "id": "6uf3SDOhMDtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_args()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "P4erIzgyKRoW",
        "outputId": "d84f4220-4c05-4b95-e90f-e0b0675d114b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--data_path [DATA_PATH]] [--seed SEED]\n",
            "                             [--dataset [DATASET]] [--verbose VERBOSE]\n",
            "                             [--epoch EPOCH] [--batch_size BATCH_SIZE]\n",
            "                             [--regs [REGS]] [--lr LR]\n",
            "                             [--embed_size EMBED_SIZE]\n",
            "                             [--weight_size [WEIGHT_SIZE]] [--core CORE]\n",
            "                             [--topk TOPK] [--lambda_coeff LAMBDA_COEFF]\n",
            "                             [--cf_model [CF_MODEL]]\n",
            "                             [--early_stopping_patience EARLY_STOPPING_PATIENCE]\n",
            "                             [--layers LAYERS] [--mess_dropout [MESS_DROPOUT]]\n",
            "                             [--sparse SPARSE] [--debug]\n",
            "                             [--loss_ratio LOSS_RATIO]\n",
            "                             [--norm_type [NORM_TYPE]] [--gpu_id GPU_ID]\n",
            "                             [--Ks [KS]] [--test_flag [TEST_FLAG]]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-21d6a4de-fcbe-441c-a723-28cfd7125bd5.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls9y_O-yJQZx"
      },
      "outputs": [],
      "source": [
        "class MICRO(nn.Module):\n",
        "    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats, text_feats):\n",
        "        super().__init__()\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.weight_size = weight_size\n",
        "        self.n_ui_layers = len(self.weight_size)\n",
        "        self.weight_size = [self.embedding_dim] + self.weight_size\n",
        "        self.user_embedding = nn.Embedding(n_users, self.embedding_dim)\n",
        "        self.item_id_embedding = nn.Embedding(n_items, self.embedding_dim)\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.item_id_embedding.weight)\n",
        "\n",
        "        if args.cf_model == 'ngcf':\n",
        "            self.GC_Linear_list = nn.ModuleList()\n",
        "            self.Bi_Linear_list = nn.ModuleList()\n",
        "            self.dropout_list = nn.ModuleList()\n",
        "            for i in range(self.n_ui_layers):\n",
        "                self.GC_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
        "                self.Bi_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
        "                self.dropout_list.append(nn.Dropout(dropout_list[i]))\n",
        "\n",
        "\n",
        "        self.image_embedding = nn.Embedding.from_pretrained(torch.Tensor(image_feats), freeze=False)\n",
        "        self.text_embedding = nn.Embedding.from_pretrained(torch.Tensor(text_feats), freeze=False)\n",
        "            \n",
        "\n",
        "        image_adj = build_sim(self.image_embedding.weight.detach())\n",
        "        image_adj = build_knn_normalized_graph(image_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
        "\n",
        "        text_adj = build_sim(self.text_embedding.weight.detach())\n",
        "        text_adj = build_knn_normalized_graph(text_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
        "\n",
        "        self.text_original_adj = text_adj.cuda()\n",
        "        self.image_original_adj = image_adj.cuda()\n",
        "        \n",
        "        self.image_trs = nn.Linear(image_feats.shape[1], args.embed_size)\n",
        "        self.text_trs = nn.Linear(text_feats.shape[1], args.embed_size)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "        self.query = nn.Sequential(\n",
        "            nn.Linear(self.embedding_dim, self.embedding_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.embedding_dim, 1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.tau = 0.5\n",
        "\n",
        "    def mm(self, x, y):\n",
        "        if args.sparse:\n",
        "            return torch.sparse.mm(x, y)\n",
        "        else:\n",
        "            return torch.mm(x, y)\n",
        "    def sim(self, z1, z2):\n",
        "        z1 = F.normalize(z1)\n",
        "        z2 = F.normalize(z2)\n",
        "        return torch.mm(z1, z2.t())\n",
        "\n",
        "    def batched_contrastive_loss(self, z1, z2, batch_size=4096):\n",
        "        device = z1.device\n",
        "        num_nodes = z1.size(0)\n",
        "        num_batches = (num_nodes - 1) // batch_size + 1\n",
        "        f = lambda x: torch.exp(x / self.tau)\n",
        "        indices = torch.arange(0, num_nodes).to(device)\n",
        "        losses = []\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            mask = indices[i * batch_size:(i + 1) * batch_size]\n",
        "            refl_sim = f(self.sim(z1[mask], z1))  # [B, N]\n",
        "            between_sim = f(self.sim(z1[mask], z2))  # [B, N]\n",
        "\n",
        "            losses.append(-torch.log(\n",
        "                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()\n",
        "                / (refl_sim.sum(1) + between_sim.sum(1)\n",
        "                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))\n",
        "                   \n",
        "        loss_vec = torch.cat(losses)\n",
        "        return loss_vec.mean()\n",
        "\n",
        "    def forward(self, adj, build_item_graph=False):\n",
        "        image_feats = self.image_trs(self.image_embedding.weight)\n",
        "        text_feats = self.text_trs(self.text_embedding.weight)\n",
        "        if build_item_graph:\n",
        "            self.image_adj = build_sim(image_feats) \n",
        "            self.image_adj = build_knn_normalized_graph(self.image_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
        "            self.image_adj = (1 - args.lambda_coeff) * self.image_adj + args.lambda_coeff * self.image_original_adj\n",
        "\n",
        "            self.text_adj = build_sim(text_feats) \n",
        "            self.text_adj = build_knn_normalized_graph(self.text_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
        "            self.text_adj = (1 - args.lambda_coeff) * self.text_adj + args.lambda_coeff * self.text_original_adj\n",
        "\n",
        "        else:\n",
        "            self.image_adj = self.image_adj.detach()\n",
        "            self.text_adj = self.text_adj.detach()\n",
        "\n",
        "        image_item_embeds = self.item_id_embedding.weight\n",
        "        text_item_embeds = self.item_id_embedding.weight\n",
        "\n",
        "        for i in range(args.layers):\n",
        "            image_item_embeds = self.mm(self.image_adj, image_item_embeds)\n",
        "\n",
        "        for i in range(args.layers):\n",
        "            text_item_embeds = self.mm(self.text_adj, text_item_embeds)  \n",
        "\n",
        "\n",
        "        att = torch.cat([self.query(image_item_embeds), self.query(text_item_embeds)], dim=-1)\n",
        "        weight = self.softmax(att)\n",
        "        h = weight[:, 0].unsqueeze(dim=1) * image_item_embeds + weight[:, 1].unsqueeze(dim=1) * text_item_embeds\n",
        "\n",
        "        \n",
        "        if args.cf_model == 'ngcf':\n",
        "            ego_embeddings = torch.cat((self.user_embedding.weight, self.item_id_embedding.weight), dim=0)\n",
        "            all_embeddings = [ego_embeddings]\n",
        "            for i in range(self.n_ui_layers):\n",
        "                side_embeddings = torch.sparse.mm(adj, ego_embeddings)\n",
        "                sum_embeddings = F.leaky_relu(self.GC_Linear_list[i](side_embeddings))\n",
        "                bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n",
        "                bi_embeddings = F.leaky_relu(self.Bi_Linear_list[i](bi_embeddings))\n",
        "                ego_embeddings = sum_embeddings + bi_embeddings\n",
        "                ego_embeddings = self.dropout_list[i](ego_embeddings)\n",
        "\n",
        "                norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
        "                all_embeddings += [norm_embeddings]\n",
        "\n",
        "            all_embeddings = torch.stack(all_embeddings, dim=1)\n",
        "            all_embeddings = all_embeddings.mean(dim=1, keepdim=False)            \n",
        "            u_g_embeddings, i_g_embeddings = torch.split(all_embeddings, [self.n_users, self.n_items], dim=0)\n",
        "            i_g_embeddings = i_g_embeddings + F.normalize(h, p=2, dim=1)\n",
        "            return u_g_embeddings, i_g_embeddings, image_item_embeds, text_item_embeds, h\n",
        "\n",
        "        elif args.cf_model == 'lightgcn': \n",
        "            ego_embeddings = torch.cat((self.user_embedding.weight, self.item_id_embedding.weight), dim=0)\n",
        "            all_embeddings = [ego_embeddings]\n",
        "            for i in range(self.n_ui_layers):\n",
        "                side_embeddings = torch.sparse.mm(adj, ego_embeddings)\n",
        "                ego_embeddings = side_embeddings\n",
        "                all_embeddings += [ego_embeddings]\n",
        "            all_embeddings = torch.stack(all_embeddings, dim=1)\n",
        "            all_embeddings = all_embeddings.mean(dim=1, keepdim=False)\n",
        "            u_g_embeddings, i_g_embeddings = torch.split(all_embeddings, [self.n_users, self.n_items], dim=0)\n",
        "            i_g_embeddings = i_g_embeddings + F.normalize(h, p=2, dim=1)\n",
        "            return u_g_embeddings, i_g_embeddings, image_item_embeds, text_item_embeds, h\n",
        "\n",
        "        elif args.cf_model == 'mf':\n",
        "                return self.user_embedding.weight, self.item_id_embedding.weight + F.normalize(h, p=2, dim=1), image_item_embeds, text_item_embeds, h\n",
        "\n",
        "\n",
        "class MF(nn.Module):\n",
        "    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats=None, text_feats=None):\n",
        "        super().__init__()\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
        "\n",
        "    def forward(self, adj, build_item_graph=False):\n",
        "        return self.user_embedding.weight, self.item_embedding.weight\n",
        "\n",
        "\n",
        "\n",
        "class NGCF(nn.Module):\n",
        "    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats=None, text_feats=None):\n",
        "        super().__init__()\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.weight_size = weight_size\n",
        "        self.n_ui_layers = len(self.weight_size)\n",
        "        self.dropout_list = nn.ModuleList()\n",
        "        self.GC_Linear_list = nn.ModuleList()\n",
        "        self.Bi_Linear_list = nn.ModuleList()\n",
        "\n",
        "        self.weight_size = [self.embedding_dim] + self.weight_size\n",
        "        for i in range(self.n_ui_layers):\n",
        "            self.GC_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
        "            self.Bi_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
        "            self.dropout_list.append(nn.Dropout(dropout_list[i]))\n",
        "\n",
        "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
        "        self.item_id_embedding = nn.Embedding(n_items, embedding_dim)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.item_id_embedding.weight)\n",
        "\n",
        "    def forward(self, adj, build_item_graph):\n",
        "        ego_embeddings = torch.cat((self.user_embedding.weight, self.item_id_embedding.weight), dim=0)\n",
        "        all_embeddings = [ego_embeddings]\n",
        "        for i in range(self.n_ui_layers):\n",
        "            side_embeddings = torch.sparse.mm(adj, ego_embeddings)\n",
        "            sum_embeddings = F.leaky_relu(self.GC_Linear_list[i](side_embeddings))\n",
        "            bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n",
        "            bi_embeddings = F.leaky_relu(self.Bi_Linear_list[i](bi_embeddings))\n",
        "            ego_embeddings = sum_embeddings + bi_embeddings\n",
        "            ego_embeddings = self.dropout_list[i](ego_embeddings)\n",
        "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
        "            all_embeddings += [norm_embeddings]\n",
        "\n",
        "        all_embeddings = torch.cat(all_embeddings, dim=1)\n",
        "        u_g_embeddings, i_g_embeddings = torch.split(all_embeddings, [self.n_users, self.n_items], dim=0)\n",
        "        return u_g_embeddings, i_g_embeddings\n",
        "\n",
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats=None, text_feats=None):\n",
        "        super().__init__()\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_ui_layers = len(weight_size)\n",
        "\n",
        "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
        "        self.item_id_embedding = nn.Embedding(n_items, embedding_dim)\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.item_id_embedding.weight)\n",
        "\n",
        "    def forward(self, adj, build_item_graph):\n",
        "        ego_embeddings = torch.cat((self.user_embedding.weight, self.item_id_embedding.weight), dim=0)\n",
        "        all_embeddings = [ego_embeddings]\n",
        "        for i in range(self.n_ui_layers):\n",
        "            side_embeddings = torch.sparse.mm(adj, ego_embeddings)\n",
        "            ego_embeddings = side_embeddings\n",
        "            all_embeddings += [ego_embeddings]\n",
        "        all_embeddings = torch.stack(all_embeddings, dim=1)\n",
        "        all_embeddings = all_embeddings.mean(dim=1, keepdim=False)\n",
        "        u_g_embeddings, i_g_embeddings = torch.split(all_embeddings, [self.n_users, self.n_items], dim=0)\n",
        "        return u_g_embeddings, i_g_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KR7-3bluR043"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}